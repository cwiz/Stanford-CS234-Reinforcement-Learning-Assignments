{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Optimal Policy for Simple MDP [20 pts]\n",
    "\n",
    "Consider the simple n-state MDP shown in Figure 1. Starting from state $s_{1}$, the agent can move\n",
    "to the right ($a_{0}$) or left ($a_{1}$) from any state $s_{i}$. Actions are deterministic and always succeed (e.g.\n",
    "going left from state $s_{2}$ goes to state $s_{1}$, and going left from state $s_{1}$ transitions to itself). Rewards\n",
    "are given upon taking an action from the state. Taking any action from the goal state $G$ earns a\n",
    "reward of $r$ = +1 and the agent stays in state $G$. Otherwise, each move has zero reward ($r$ = 0).\n",
    "Assume a discount factor $\\gamma$ < 1.\n",
    "\n",
    "![Figure-1](images/figure-1.png \"Figure-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Question:**\n",
    "\n",
    "The optimal action from any state $s_{i}$ is taking $a_{0}$ (right) until the agent reaches the goal state\n",
    "$G$. Find the optimal value function for all states $s_{i}$ and the goal state $G$. [5 pts]\n",
    "\n",
    "**(a) Answer:**\n",
    "\n",
    "*Def.* $V$ expectation of return if agent follows policy $\\pi$ starting from state $s$ and taking action $a$\n",
    "\n",
    "```\n",
    "First we would set up a table of numbers, one for each possible state\n",
    "of the game. Each number will be the latest estimate of the probability of our winning\n",
    "from that state. We treat this estimate as the stateâ€™s value, and the whole table is the\n",
    "learned value function.\n",
    "``` \n",
    "\n",
    "Sutton, Barto, page 9\n",
    "\n",
    "| $s_{1}$ | $s_{n-2}$ | $s_{n-1}$ |\n",
    "|---------|-----------|-----------|\n",
    "| 1       | 1         | 1         |\n",
    "\n",
    "Agent's probability to win is 100% because it doesn't depend in which state he starts he is going to reach terminal state always moving right.\n",
    "\n",
    "**(b) Question:**\n",
    "\n",
    "Does the optimal policy depend on the value of the discount factor \n",
    "? Explain your answer. [5 pts]\n",
    "\n",
    "**(b) Answer:**\n",
    "\n",
    "It shouldn't unless discount factor is zero. Value function denotes probablity of winnign starting from state $s_{i}$ and agent should win in all cases in this example. \n",
    "\n",
    "**(d) Question:**\n",
    "\n",
    "After adding a constant $c$ to all rewards now consider scaling all the rewards by a constant a\n",
    "(i.e. $r_{new} = a(c + r_{old}))$. Find the new optimal value function for all states $s_{i}$ and the goal\n",
    "state $G$. Does that change the optimal policy? Explain your answer, If yes, give an example\n",
    "of a and c that changes the optimal policy. [5 pts]\n",
    "\n",
    "**(d) Answer:**\n",
    "\n",
    "We could reverse the game, by making c = -1 for any negative a. That would make state g unfeasible with 0 reward and any other state have positive reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Running Time of Value Iteration [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we construct an example to bound the number of steps it will take to find the optimal\n",
    "policy using value iteration. Consider the infinite MDP with discount factor $\\gamma$ < 1 illustrated in\n",
    "Figure 2. It consists of 3 states, and rewards are given upon taking an action from the state. From\n",
    "state $s_{0}$, action $a_{1}$ has zero immediate reward and causes a deterministic transition to state $s_{1}$ where\n",
    "there is reward +1 for every time step afterwards (regardless of action). From state $s_{0}$, action $a_{2}$ causes a deterministic transition to state s2 with immediate reward of $\\gamma^{2}/1-\\gamma$  but state $s_{2}$ has\n",
    "zero reward for every time step afterwards (regardless of action).\n",
    "\n",
    "![Figure-2](images/figure-2.png \"Figure-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Question:**\n",
    "\n",
    "What is the total discounted return $\\sum\\gamma^{t}r_{t}$ of taking action $a_{1}$ from state $s_{0}$ at time step\n",
    "t = 0? [5 pts]\n",
    "\n",
    "**(a) Answer:**\n",
    "\n",
    "$\\sum\\gamma^{t}$\n",
    "\n",
    "**(a) Correction:**\n",
    "Above equals to $\\gamma/(1-\\gamma)$\n",
    "\n",
    "**(b) Question:**\n",
    "\n",
    "(b) What is the total discounted return $\\sum\\gamma^{t}r_{t}$ of taking action $a_{2}$ from state $s_{0}$ at time step\n",
    "t = 0? What is the optimal action? [5 pts]\n",
    "\n",
    "$\\gamma^{2}/(1-\\gamma)$\n",
    "\n",
    "Optimal action maximizes expected sum of returns. $a_{1} > a_{2}$ for all positive $\\gamma$\n",
    "\n",
    "**(c) Question:**\n",
    "\n",
    "![Figure-3](images/q-2-c.png \"Question 2 C\")\n",
    "\n",
    "**(c) Answer:**\n",
    "\n",
    "```\n",
    "Value iteration is a method of computing an optimal MDP policy and its value.\n",
    "\n",
    "Value iteration starts at the end and then works backward, refining an estimate of either Q* or V*. There is really no end, so it uses an arbitrary end point. \n",
    "``` \n",
    "\n",
    "https://artint.info/html/ArtInt_227.html\n",
    "\n",
    "**(c) Correction:**\n",
    "\n",
    "![Figure-4](images/solution-2-c.png \"Question 2 C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
